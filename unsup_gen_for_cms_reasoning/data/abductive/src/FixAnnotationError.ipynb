{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8b9df0-63ff-44c5-ad11-bf40d4f8b196",
   "metadata": {},
   "source": [
    "See annotation.readme for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ef7e9d-1eef-4889-8803-60186fa6c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebaf5b8e-77ce-4ade-988f-1bf0a4e97b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/1003_annotations.pickle', 'rb') as f:\n",
    "    annotation = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440b3366-3eb0-4de7-9267-0e805db602b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(annotation):\n",
    "    annotation[i]['entry']['preprocess'] = re.sub('\\r\\n', '\\n', d['entry']['preprocess'])\n",
    "    if not annotation[i]['entry']['preprocess'].endswith('\\n'):\n",
    "        annotation[i]['entry']['preprocess'] += '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985567e6-267c-4658-8707-9d3173e73639",
   "metadata": {},
   "source": [
    "# Fix errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd22997c-4418-42a6-94aa-61a5dfe2465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '38'}, {'tokens': 'her', 'token_list': ['her'], 'group_num': '', 'tokens_id': '13'}, {'tokens': 'beauty under the tree', 'token_list': ['beauty', 'under', 'the', 'tree'], 'group_num': '', 'tokens_id': '22-25'}]\n",
      "After:\n",
      "[{'tokens': 'her', 'token_list': ['her'], 'group_num': '', 'tokens_id': '13'}, {'tokens': 'beauty under the tree', 'token_list': ['beauty', 'under', 'the', 'tree'], 'group_num': '', 'tokens_id': '22-25'}]\n"
     ]
    }
   ],
   "source": [
    "# Remove 'S2' from subjects of event[0] in dialog[20] turn[3]\n",
    "print('Before:')\n",
    "print(annotation[20]['with_token_anno'][3]['triples'][0]['subject'])\n",
    "annotation[20]['with_token_anno'][3]['triples'][0]['subject'].pop(0)\n",
    "print('After:')\n",
    "print(annotation[20]['with_token_anno'][3]['triples'][0]['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53447843-b6c0-4f69-baff-a793a54a7bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'token_list': ['us'], 'group_num': '', 'tokens': 'us', 'tokens_id': '10'}]\n",
      "After:\n",
      "[{'token_list': ['us'], 'group_num': '', 'tokens': 'us', 'tokens_id': '10'}, {'tokens': 'S1', 'token_list': ['S1'], 'group_num': '', 'tokens_id': '0'}]\n"
     ]
    }
   ],
   "source": [
    "# Add 'S1' to the event in 3rd turn \n",
    "# Turn 2: Ingestion: ('S2',)|('12',) having with #1 (4,9#1) ('us', 'S1')|('10', '0') \n",
    "# Turn 3: Ingestion: ('S2',)|('12',) having with#1 (4,9#1) ('us',)|('10',) \n",
    "# Ignore other examples like this and deal with this problem in the clean_data.py: convert_dic\n",
    "ex = annotation[22]['with_token_anno'][3]['triples'][1]['object']\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex.append({'tokens': 'S1', 'token_list': ['S1'], 'group_num': '', 'tokens_id': '0'})\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff99981-ad92-454f-8216-11644883a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'eventType': 'explicit', 'polarity': 'pos', 'modality': 'actual', 'time': 'NOW', 'predicate': {'tokens': 'meet', 'token_list': ['meet'], 'group_num': '', 'tokens_id': '23'}, 'subject': [{'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '17'}], 'object': [{'tokens': 'you', 'token_list': ['you'], 'group_num': '', 'tokens_id': '24'}, {'tokens': 'S1', 'token_list': ['S1'], 'group_num': '', 'tokens_id': '0'}, {'tokens': 'Mr . Cooper', 'token_list': ['Mr', '.', 'Cooper'], 'group_num': '', 'tokens_id': '26-28'}], 'frame_name': 'Make_acquaintance', 'frame_candidates': ['Assemble', 'Meet_specifications', 'Make_acquaintance', 'Response', 'Meet_with_response', 'Locative_relation', 'Come_together']}, {'who': 'S2', 'eventType': 'implicit', 'subject': [{'token_list': ['You'], 'group_num': '', 'tokens': 'You', 'tokens_id': '41'}, {'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '17'}, {'token_list': ['Ms', '.', 'Wang'], 'group_num': '', 'tokens': 'Ms . Wang', 'tokens_id': '37-39'}], 'object': [{'token_list': ['Canada'], 'group_num': '', 'tokens': 'Canada', 'tokens_id': '50'}], 'predicate': {'tokens': 'travel from', 'tokens_id': '', 'token_list': ['travel', 'from'], 'group_num': ''}, 'time': 'BEFORE', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Travel', 'Motion'], 'frame_name': 'Travel'}]\n",
      "After:\n",
      "[{'eventType': 'explicit', 'polarity': 'pos', 'modality': 'actual', 'time': 'NOW', 'predicate': {'tokens': 'meet', 'token_list': ['meet'], 'group_num': '', 'tokens_id': '23'}, 'subject': [{'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '17'}], 'object': [{'tokens': 'you', 'token_list': ['you'], 'group_num': '', 'tokens_id': '24'}, {'tokens': 'S1', 'token_list': ['S1'], 'group_num': '', 'tokens_id': '0'}, {'tokens': 'Mr . Cooper', 'token_list': ['Mr', '.', 'Cooper'], 'group_num': '', 'tokens_id': '26-28'}], 'frame_name': 'Make_acquaintance', 'frame_candidates': ['Assemble', 'Meet_specifications', 'Make_acquaintance', 'Response', 'Meet_with_response', 'Locative_relation', 'Come_together']}]\n",
      "Before:\n",
      "{'eventType': 'implicit', 'polarity': 'pos', 'modality': 'actual', 'time': 'BEFORE', 'predicate': {'tokens': 'travel', 'token_list': ['travel'], 'group_num': '', 'tokens_id': ''}, 'subject': [{'tokens': 'You', 'token_list': ['You'], 'group_num': '', 'tokens_id': '41'}, {'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '17'}, {'tokens': 'Ms . Wang', 'token_list': ['Ms', '.', 'Wang'], 'group_num': '', 'tokens_id': '37-39'}], 'object': [{'tokens': 'from Canada', 'token_list': ['from', 'Canada'], 'group_num': '', 'tokens_id': '49-50'}], 'frame_name': 'Motion', 'frame_candidates': ['Travel', 'Motion']}\n",
      "After:\n",
      "{'eventType': 'implicit', 'polarity': 'pos', 'modality': 'actual', 'time': 'BEFORE', 'predicate': {'tokens': 'travel', 'token_list': ['travel'], 'group_num': '', 'tokens_id': ''}, 'subject': [{'tokens': 'You', 'token_list': ['You'], 'group_num': '', 'tokens_id': '41'}, {'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '17'}, {'tokens': 'Ms . Wang', 'token_list': ['Ms', '.', 'Wang'], 'group_num': '', 'tokens_id': '37-39'}], 'object': [{'tokens': 'from Canada', 'token_list': ['from', 'Canada'], 'group_num': '', 'tokens_id': '49-50'}], 'frame_name': 'Travel', 'frame_candidates': ['Travel', 'Motion']}\n"
     ]
    }
   ],
   "source": [
    "# Remove ['travel from'] ['Canada'] which is duplicated with ['travel' 'from Canada']\n",
    "ex = annotation[28]['with_token_anno'][2]['triples']\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex.pop(1)\n",
    "print('After:')\n",
    "print(ex)\n",
    "\n",
    "# Change framename from 'Motion' to 'Travel'\n",
    "ex = annotation[28]['with_token_anno'][3]['triples'][1]\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex['frame_name'] = ex['frame_name'].replace('Motion', 'Travel')\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d370436-e754-4841-af95-74818e703ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "{'token_list': ['our', 'company'], 'group_num': '', 'tokens': 'our company', 'tokens_id': '13-14'}\n",
      "After:\n",
      "{'token_list': ['by', 'our', 'company'], 'group_num': '', 'tokens': 'by our company', 'tokens_id': '12-14'}\n"
     ]
    }
   ],
   "source": [
    "# Make \"our company\" --> \"by our company\" to match other entities\n",
    "ex = annotation[34]['with_token_anno'][0]['triples'][2]['object'][0]\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex['token_list'] = ['by'] + ex['token_list']\n",
    "ex['tokens'] = 'by ' + ex['tokens']\n",
    "ex['tokens_id'] = ex['tokens_id'].replace('3','2')\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9bfe165-68b7-4eb4-a73c-24e8d7243593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokens': 'the position of an usher in your restaurant', 'token_list': ['the', 'position', 'of', 'an', 'usher', 'in', 'your', 'restaurant'], 'group_num': '', 'tokens_id': '9-16'}, {'tokens': 'S2(your)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '18(15)'}]\n",
      "Before:\n",
      "[{'tokens': 'the position of an usher in your restaurant', 'token_list': ['the', 'position', 'of', 'an', 'usher', 'in', 'your', 'restaurant'], 'group_num': '', 'tokens_id': '9-16'}]\n",
      "After:\n",
      "[{'tokens': 'the position of an usher in your restaurant', 'token_list': ['the', 'position', 'of', 'an', 'usher', 'in', 'your', 'restaurant'], 'group_num': '', 'tokens_id': '9-16'}, {'tokens': 'S2(your)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '18(15)'}]\n"
     ]
    }
   ],
   "source": [
    "# Sync two events\n",
    "ex = annotation[36]['with_token_anno'][1]['triples'][0]['object']\n",
    "print(ex)\n",
    "ex = annotation[36]['with_token_anno'][0]['triples'][0]['object']\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex = annotation[36]['with_token_anno'][1]['triples'][0]['object']\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7dd770d-2a9a-45f8-bde5-0cc1bb3c5efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '9'}, {'tokens': 'Peter', 'token_list': ['Peter'], 'group_num': '', 'tokens_id': '2'}]\n",
      "Before:\n",
      "[{'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '9'}]\n",
      "After:\n",
      "[{'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '9'}, {'tokens': 'Peter', 'token_list': ['Peter'], 'group_num': '', 'tokens_id': '2'}]\n"
     ]
    }
   ],
   "source": [
    "# Sync two events\n",
    "ex = annotation[40]['with_token_anno'][1]['triples'][0]['subject']\n",
    "print(ex)\n",
    "ex = annotation[40]['with_token_anno'][2]['triples'][0]['subject']\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex = annotation[40]['with_token_anno'][1]['triples'][0]['subject']\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "236f883b-ef95-4e86-8e25-87731eef821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "{'tokens': 'flight for the 22', 'token_list': ['flight', 'for', 'the', '22'], 'group_num': '', 'tokens_id': '57-59, 83'}\n",
      "After:\n",
      "{'tokens': 'flight for the 22', 'token_list': ['flight', 'for', 'the', '22'], 'group_num': '', 'tokens_id': '57-59,83'}\n"
     ]
    }
   ],
   "source": [
    "# Fix tokens_id:'57-59, 83' --> '57-59,83'\n",
    "ex = annotation[84]['with_token_anno'][4]['triples'][0]['object'][0]\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex['tokens_id'] = '57-59,83'\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c938104b-3145-466e-9d35-76da8e1b126e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "{'tokens': 'Hello , Mr . Smith', 'token_list': ['Hello', ',', 'Mr', '.', 'Smith'], 'group_num': '', 'tokens_id': '2-6'}\n",
      "After:\n",
      "{'tokens': 'Mr . Smith', 'token_list': ['Mr', '.', 'Smith'], 'group_num': '', 'tokens_id': '4-6'}\n"
     ]
    }
   ],
   "source": [
    "# remove \"Hello , \" to match other entities\n",
    "ex = annotation[93]['with_token_anno']\n",
    "print('Before:')\n",
    "print(ex[1]['triples'][0]['object'][1])\n",
    "for turn in ex:\n",
    "    for triple in turn['triples']:\n",
    "        for obj in triple['object']:\n",
    "            if 'Hello , Mr . Smith' in obj['tokens']:\n",
    "                obj['token_list'] = ['Mr', '.', 'Smith']\n",
    "                obj['tokens'] = 'Mr . Smith'\n",
    "                obj['tokens_id'] = '4-6'\n",
    "print('After:')\n",
    "print(ex[1]['triples'][0]['object'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65fc7831-6341-4631-b3b6-413a9291670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "[{'tokens': 'spend the weekend with us', 'token_list': ['spend', 'the', 'weekend', 'with', 'us'], 'group_num': '', 'tokens_id': '6-10'}, {'tokens': 'S2(S1)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '12(0)'}]\n",
      "[{'tokens': 'spend the weekend with us', 'token_list': ['spend', 'the', 'weekend', 'with', 'us'], 'group_num': '', 'tokens_id': '6-10'}, {'tokens': 'S2(S1)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '12(0)'}]\n",
      "[{'tokens': 'spend the weekend with us', 'token_list': ['spend', 'the', 'weekend', 'with', 'us'], 'group_num': '', 'tokens_id': '6-10'}, {'tokens': 'S2(S1)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '12(0)'}]\n",
      "After\n",
      "[{'tokens': 'spend the weekend with us', 'token_list': ['spend', 'the', 'weekend', 'with', 'us'], 'group_num': '', 'tokens_id': '6-10'}, {'tokens': 'S1(us)', 'token_list': 'S1', 'group_num': '', 'tokens_id': '0(10)'}]\n",
      "[{'tokens': 'spend the weekend with us', 'token_list': ['spend', 'the', 'weekend', 'with', 'us'], 'group_num': '', 'tokens_id': '6-10'}, {'tokens': 'S1(us)', 'token_list': 'S1', 'group_num': '', 'tokens_id': '0(10)'}]\n",
      "[{'tokens': 'spend the weekend with us', 'token_list': ['spend', 'the', 'weekend', 'with', 'us'], 'group_num': '', 'tokens_id': '6-10'}, {'tokens': 'S1(us)', 'token_list': 'S1', 'group_num': '', 'tokens_id': '0(10)'}]\n"
     ]
    }
   ],
   "source": [
    "# Replace ['spend the weekend with us', 'S2(S1)'], tokens_id: ('6-10', '12(0)')\n",
    "# as ['spend the weekend with us', 'S1(us)'], tokens_id: ('6-10', '0(10)')\n",
    "ex = annotation[127]['with_token_anno']\n",
    "new_obj = {\n",
    "    'tokens': 'S2(us)',\n",
    "    'token_list': ['S2'],\n",
    "    'group_num': '',\n",
    "    'tokens_id': '12(10)'\n",
    "}\n",
    "print('Before')\n",
    "for i in range(1,4):\n",
    "    print(ex[i]['triples'][0]['object'])\n",
    "    ex[i]['triples'][0]['object'][1]['tokens'] = 'S1(us)'\n",
    "    ex[i]['triples'][0]['object'][1]['token_list'] = 'S1'\n",
    "    ex[i]['triples'][0]['object'][1]['tokens_id'] = '0(10)'\n",
    "    \n",
    "print('After')\n",
    "for i in range(1,4):\n",
    "    print(ex[i]['triples'][0]['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10755fc0-9a02-4545-8271-f62d826de2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove space in tokens_id\n",
    "# 14_3_2 {'tokens': 'try on', 'token_list': ['try', 'on'], 'group_num': '', 'tokens_id': '40, 42'}\n",
    "# 94_3_3 {'tokens': 'convey to', 'token_list': ['convey', 'to'], 'group_num': '#3', 'tokens_id': '89, 92'}\n",
    "# 94_4_3 {'tokens': 'convey to', 'token_list': ['convey', 'to'], 'group_num': '#3', 'tokens_id': '89, 92'}\n",
    "\n",
    "# 213\n",
    "# Predicate: {'token_list': ['been', 'with', 'the', 'company'], 'group_num': '', 'tokens': 'been with the company', 'tokens_id': '51-54'}\n",
    "# make \"been\" the predicate and \"with the company\" the object\n",
    "\n",
    "# 214\n",
    "# Predicate: {'token_list': ['make', 'a', 'decision'], 'group_num': '', 'tokens': 'make a decision', 'tokens_id': '27-29'}\n",
    "# make \"make\" the predicate and \"a decision\" the object\n",
    "\n",
    "# 234\n",
    "# Predicate: {'token_list': ['give', 'us'], 'group_num': '#1', 'tokens': 'give us#1', 'tokens_id': '38-39'}\n",
    "# make \"give\" the predicate and \"us\" the object\n",
    "\n",
    "# 284\n",
    "# Predicate: {'token_list': ['dropped', 'it'], 'group_num': '#1', 'tokens': 'dropped it#1', 'tokens_id': '91-92'}\n",
    "# \"dropped\" -> predicate; \"it\" -> object\n",
    "\n",
    "# 285\n",
    "# Predicate: {'token_list': ['helps', 'you'], 'group_num': '#1', 'tokens': 'helps you#1', 'tokens_id': '220-221'}\n",
    "# \"helps\" -> predicate, \"you\" -> object\n",
    "\n",
    "# 299\n",
    "# Predicate: {'tokens': 'take a tour', 'token_list': ['take', 'a', 'tour'], 'group_num': '', 'tokens_id': '16-18'}\n",
    "# \"take\" -> predicate, \"a tour\"-> object\n",
    "\n",
    "# 366\n",
    "# Predicate: {'tokens': 'be a member of', 'token_list': ['be', 'a', 'member', 'of'], 'group_num': '', 'tokens_id': '32-35'}\n",
    "\n",
    "# 383\n",
    "# Predicate: {'token_list': ['told', 'me', 'that'], 'group_num': '#1', 'tokens': 'told me that#1', 'tokens_id': '4-6'}\n",
    "# Predicate: {'token_list': ['plays', 'the', 'field'], 'group_num': '', 'tokens': 'plays the field', 'tokens_id': '74-76'}\n",
    "\n",
    "# 567\n",
    "# Predicate: {'tokens': 'do exercise', 'token_list': ['do', 'exercise'], 'group_num': '', 'tokens_id': '86,88'}\n",
    "# \"do\" -> predicate, \"more exercise\" -> object\n",
    "\n",
    "# 573 Predicate: {'tokens': 'surf the Internet', 'token_list': ['surf', 'the', 'Internet'], 'group_num': '', 'tokens_id': '5-7'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "429c11e9-9640-403f-8f77-fa31cde40dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'tokens': 'the samples', 'token_list': ['the', 'samples'], 'group_num': '', 'tokens_id': '41-42'}, {'tokens': 'you(S1)', 'token_list': ['you'], 'group_num': '', 'tokens_id': '44(46)'}]\n",
      "After:\n",
      "[{'tokens': 'the samples', 'token_list': ['the', 'samples'], 'group_num': '', 'tokens_id': '41-42'}]\n",
      "Before:\n",
      "[{'tokens': 'the samples', 'token_list': ['the', 'samples'], 'group_num': '', 'tokens_id': '41-42'}, {'tokens': 'you(S1)', 'token_list': ['you'], 'group_num': '', 'tokens_id': '44(46)'}]\n",
      "After:\n",
      "[{'tokens': 'the samples', 'token_list': ['the', 'samples'], 'group_num': '', 'tokens_id': '41-42'}]\n"
     ]
    }
   ],
   "source": [
    "# Remove 'you(S1),44(46)' which is duplicated and in the wrong order which causes error\n",
    "# Giving: ['me,38', 'S2,33'] leave #1 (40#1) ['the samples,41-42', 'you(S1),44(46)'] \n",
    "ex = annotation[214]['with_token_anno'][2]['triples'][2]\n",
    "print('Before:')\n",
    "print(ex['object'])\n",
    "ex['object'].pop(1)\n",
    "print('After:')\n",
    "print(ex['object'])\n",
    "\n",
    "ex = annotation[214]['with_token_anno'][3]['triples'][2]\n",
    "print('Before:')\n",
    "print(ex['object'])\n",
    "ex['object'].pop(1)\n",
    "print('After:')\n",
    "print(ex['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f6ca20-126a-4575-bd3c-fe140fa82738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'token_list': ['sure', 'you', 'can', 'do', 'it'], 'group_num': '', 'tokens': 'sure you can do it', 'tokens_id': '48-52'}, {'token_list': [''], 'group_num': '', 'tokens': '(S2)', 'tokens_id': '(26)49'}]\n",
      "After:\n",
      "[{'token_list': ['sure', 'you', 'can', 'do', 'it'], 'group_num': '', 'tokens': 'sure you can do it', 'tokens_id': '48-52'}, {'token_list': ['S2'], 'group_num': '', 'tokens': 'S2(you)', 'tokens_id': '26(49)'}]\n"
     ]
    }
   ],
   "source": [
    "# Add missing token and fix the order of tokens_id\n",
    "# [{'token_list': ['sure', 'you', 'can', 'do', 'it'], 'group_num': '', 'tokens': 'sure you can do it', 'tokens_id': '48-52'}, \n",
    "#  {'token_list': [''], 'group_num': '', 'tokens': '(S2)', 'tokens_id': '(26)49'}]\n",
    "ex = annotation[219]['with_token_anno'][4]\n",
    "print('Before:')\n",
    "print(ex['triples'][1]['object'])\n",
    "ex['triples'][1]['object'][1]['token_list'] = ['S2']\n",
    "ex['triples'][1]['object'][1]['tokens'] = 'S2(you)'\n",
    "ex['triples'][1]['object'][1]['tokens_id'] = '26(49)'\n",
    "print('After:')\n",
    "print(ex['triples'][1]['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4ff609-816e-419f-a77c-400a311c2c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "{'who': 'Both', 'eventType': 'explicit', 'subject': [{'token_list': ['We'], 'group_num': '', 'tokens': 'We', 'tokens_id': '33'}, {'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '15'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}], 'object': [{'token_list': ['like', 'bees', 'in', 'the', 'whole', 'working', 'days'], 'group_num': '', 'tokens': 'like bees in the whole working days', 'tokens_id': '35-41'}], 'predicate': {'token_list': ['worked'], 'group_num': '', 'tokens': 'worked', 'tokens_id': '34'}, 'time': 'BEFORE', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Usefulness', 'Work', 'Being_operational', 'Being_employed', 'Working_a_post'], 'frame_name': 'Usefulness'}\n",
      "After\n",
      "{'who': 'Both', 'eventType': 'explicit', 'subject': [{'token_list': ['We'], 'group_num': '', 'tokens': 'We', 'tokens_id': '33'}, {'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '15'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}], 'object': [{'token_list': ['like', 'bees', 'in', 'the', 'whole', 'working', 'days'], 'group_num': '', 'tokens': 'like bees in the whole working days', 'tokens_id': '35-41'}], 'predicate': {'token_list': ['worked'], 'group_num': '', 'tokens': 'worked', 'tokens_id': '34'}, 'time': 'BEFORE', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Usefulness', 'Work', 'Being_operational', 'Being_employed', 'Working_a_post'], 'frame_name': 'Work'}\n",
      "\n",
      "Before\n",
      "{'who': 'Both', 'eventType': 'explicit', 'subject': [{'token_list': ['We'], 'group_num': '', 'tokens': 'We', 'tokens_id': '33'}, {'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '15'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}], 'object': [{'token_list': ['like', 'bees', 'in', 'the', 'whole', 'working', 'days'], 'group_num': '', 'tokens': 'like bees in the whole working days', 'tokens_id': '35-41'}], 'predicate': {'token_list': ['worked'], 'group_num': '', 'tokens': 'worked', 'tokens_id': '34'}, 'time': 'BEFORE', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Usefulness', 'Work', 'Being_operational', 'Being_employed', 'Working_a_post'], 'frame_name': 'Usefulness'}\n",
      "After\n",
      "{'who': 'Both', 'eventType': 'explicit', 'subject': [{'token_list': ['We'], 'group_num': '', 'tokens': 'We', 'tokens_id': '33'}, {'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '15'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}], 'object': [{'token_list': ['like', 'bees', 'in', 'the', 'whole', 'working', 'days'], 'group_num': '', 'tokens': 'like bees in the whole working days', 'tokens_id': '35-41'}], 'predicate': {'token_list': ['worked'], 'group_num': '', 'tokens': 'worked', 'tokens_id': '34'}, 'time': 'BEFORE', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Usefulness', 'Work', 'Being_operational', 'Being_employed', 'Working_a_post'], 'frame_name': 'Work'}\n",
      "\n",
      "Before\n",
      "{'who': 'Both', 'eventType': 'explicit', 'subject': [{'token_list': ['We'], 'group_num': '', 'tokens': 'We', 'tokens_id': '33'}, {'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '15'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}], 'object': [{'token_list': ['like', 'bees', 'in', 'the', 'whole', 'working', 'days'], 'group_num': '', 'tokens': 'like bees in the whole working days', 'tokens_id': '35-41'}], 'predicate': {'token_list': ['worked'], 'group_num': '', 'tokens': 'worked', 'tokens_id': '34'}, 'time': 'BEFORE', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Usefulness', 'Work', 'Being_operational', 'Being_employed', 'Working_a_post'], 'frame_name': 'Usefulness'}\n",
      "After\n",
      "{'who': 'Both', 'eventType': 'explicit', 'subject': [{'token_list': ['We'], 'group_num': '', 'tokens': 'We', 'tokens_id': '33'}, {'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '15'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}], 'object': [{'token_list': ['like', 'bees', 'in', 'the', 'whole', 'working', 'days'], 'group_num': '', 'tokens': 'like bees in the whole working days', 'tokens_id': '35-41'}], 'predicate': {'token_list': ['worked'], 'group_num': '', 'tokens': 'worked', 'tokens_id': '34'}, 'time': 'BEFORE', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Usefulness', 'Work', 'Being_operational', 'Being_employed', 'Working_a_post'], 'frame_name': 'Work'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change framename from \"Usefulness\" to \"Work\"\n",
    "for i in range(3,6):\n",
    "    ex = annotation[374]['with_token_anno'][i]['triples'][0]\n",
    "    print('Before')\n",
    "    print(ex)\n",
    "    ex['frame_name'] = 'Work'\n",
    "    print('After')\n",
    "    print(ex)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "382ae0cc-e7cc-42b3-8868-6182231659f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'tokens': 'your friends', 'token_list': ['your', 'friends'], 'group_num': '', 'tokens_id': '55-56'}, {'tokens': 'your(S1)', 'token_list': ['your'], 'group_num': '', 'tokens_id': '(58)55'}]\n",
      "[{'tokens': 'your friends', 'token_list': ['your', 'friends'], 'group_num': '', 'tokens_id': '55-56'}, {'tokens': 'your(S1)', 'token_list': ['your'], 'group_num': '', 'tokens_id': '(58)55'}]\n",
      "After:\n",
      "[{'tokens': 'your friends', 'token_list': ['your', 'friends'], 'group_num': '', 'tokens_id': '55-56'}, {'tokens': 'S1(your)', 'token_list': 'S1', 'group_num': '', 'tokens_id': '58(55)'}]\n",
      "[{'tokens': 'your friends', 'token_list': ['your', 'friends'], 'group_num': '', 'tokens_id': '55-56'}, {'tokens': 'S1(your)', 'token_list': 'S1', 'group_num': '', 'tokens_id': '58(55)'}]\n"
     ]
    }
   ],
   "source": [
    "# your(S1) --> S1(your)\n",
    "ex = annotation[408]['with_token_anno']\n",
    "print('Before:')\n",
    "print(ex[2]['triples'][-1]['object'])\n",
    "print(ex[3]['triples'][-1]['object'])\n",
    "\n",
    "ex[2]['triples'][-1]['object'][1]['tokens'] = 'S1(your)'\n",
    "ex[2]['triples'][-1]['object'][1]['token_list'] = 'S1'\n",
    "ex[2]['triples'][-1]['object'][1]['tokens_id'] = '58(55)'\n",
    "ex[3]['triples'][-1]['object'][1]['tokens'] = 'S1(your)'\n",
    "ex[3]['triples'][-1]['object'][1]['token_list'] = 'S1'\n",
    "ex[3]['triples'][-1]['object'][1]['tokens_id'] = '58(55)'\n",
    "\n",
    "print('After:')\n",
    "print(ex[2]['triples'][-1]['object'])\n",
    "print(ex[3]['triples'][-1]['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3307550e-05a6-4ee8-8443-53b67c34ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "{'token_list': ['have', 'dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'have dinner Saturday night', 'tokens_id': '6,11-13'}\n",
      "After:\n",
      "{'token_list': ['dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'dinner Saturday night', 'tokens_id': '11-13'}\n",
      "Before:\n",
      "{'token_list': ['have', 'dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'have dinner Saturday night', 'tokens_id': '6,11-13'}\n",
      "After:\n",
      "{'token_list': ['dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'dinner Saturday night', 'tokens_id': '11-13'}\n",
      "Before:\n",
      "{'token_list': ['have', 'dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'have dinner Saturday night', 'tokens_id': '6,11-13'}\n",
      "After:\n",
      "{'token_list': ['dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'dinner Saturday night', 'tokens_id': '11-13'}\n",
      "Before:\n",
      "{'token_list': ['have', 'dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'have dinner Saturday night', 'tokens_id': '6,11-13'}\n",
      "After:\n",
      "{'token_list': ['dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'dinner Saturday night', 'tokens_id': '11-13'}\n",
      "Before:\n",
      "{'token_list': ['have', 'dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'have dinner Saturday night', 'tokens_id': '6,11-13'}\n",
      "After:\n",
      "{'token_list': ['dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'dinner Saturday night', 'tokens_id': '11-13'}\n",
      "Before:\n",
      "{'token_list': ['have', 'dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'have dinner Saturday night', 'tokens_id': '6,11-13'}\n",
      "After:\n",
      "{'token_list': ['dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'dinner Saturday night', 'tokens_id': '11-13'}\n",
      "Before:\n",
      "{'token_list': ['have', 'dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'have dinner Saturday night', 'tokens_id': '6,11-13'}\n",
      "After:\n",
      "{'token_list': ['dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'dinner Saturday night', 'tokens_id': '11-13'}\n"
     ]
    }
   ],
   "source": [
    "# remove \"have\" from the object\n",
    "# {'token_list': ['have', 'dinner', 'Saturday', 'night'], 'group_num': '', 'tokens': 'have dinner Saturday night', 'tokens_id': '6,11-13'}\n",
    "for i in range(4):\n",
    "    ex = annotation[413]['with_token_anno'][i]['triples'][3]['object'][0]\n",
    "    print('Before:')\n",
    "    print(ex)\n",
    "    ex['token_list'] = ['dinner', 'Saturday', 'night']\n",
    "    ex['tokens'] = 'dinner Saturday night'\n",
    "    ex['tokens_id'] = '11-13'\n",
    "    print('After:')\n",
    "    print(ex)\n",
    "\n",
    "for i in range(1, 4):\n",
    "    ex = annotation[413]['with_token_anno'][i]['triples'][5]['object'][0]\n",
    "    print('Before:')\n",
    "    print(ex)\n",
    "    ex['token_list'] = ['dinner', 'Saturday', 'night']\n",
    "    ex['tokens'] = 'dinner Saturday night'\n",
    "    ex['tokens_id'] = '11-13'\n",
    "    print('After:')\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ef36bb8-f764-48af-9a0d-bf5bc6f059d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'tokens': 'my wife ’ s birthday party', 'token_list': ['my', 'wife', '’', 's', 'birthday', 'party'], 'group_num': '', 'tokens_id': '12-17'}, {'tokens': 'Frank(my)', 'token_list': ['Frank'], 'group_num': '', 'tokens_id': '23(12)'}, {'tokens': 'S2(my)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '19(12)'}]\n",
      "[{'tokens': 'my wife ’ s birthday party', 'token_list': ['my', 'wife', '’', 's', 'birthday', 'party'], 'group_num': '', 'tokens_id': '12-17'}, {'tokens': 'Frank(my)', 'token_list': ['Frank'], 'group_num': '', 'tokens_id': '23(12)'}, {'tokens': 'S2(my)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '19(12)'}]\n",
      "[{'tokens': 'my wife ’ s birthday party', 'token_list': ['my', 'wife', '’', 's', 'birthday', 'party'], 'group_num': '', 'tokens_id': '12-17'}, {'tokens': 'Frank(my)', 'token_list': ['Frank'], 'group_num': '', 'tokens_id': '23(12)'}, {'tokens': 'S2(my)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '19(12)'}]\n",
      "After:\n",
      "[{'tokens': 'my wife ’ s birthday party', 'token_list': ['my', 'wife', '’', 's', 'birthday', 'party'], 'group_num': '', 'tokens_id': '12-17'}, {'tokens': 'Frank(my)', 'token_list': ['Frank'], 'group_num': '', 'tokens_id': '23(12)'}]\n",
      "[{'tokens': 'my wife ’ s birthday party', 'token_list': ['my', 'wife', '’', 's', 'birthday', 'party'], 'group_num': '', 'tokens_id': '12-17'}, {'tokens': 'Frank(my)', 'token_list': ['Frank'], 'group_num': '', 'tokens_id': '23(12)'}]\n",
      "[{'tokens': 'my wife ’ s birthday party', 'token_list': ['my', 'wife', '’', 's', 'birthday', 'party'], 'group_num': '', 'tokens_id': '12-17'}, {'tokens': 'Frank(my)', 'token_list': ['Frank'], 'group_num': '', 'tokens_id': '23(12)'}]\n"
     ]
    }
   ],
   "source": [
    "# remove S2(my) from ['my wife ’ s birthday party,12-17', 'Frank(my),23(12)', 'S2(my),19(12)']\n",
    "ex = annotation[424]['with_token_anno']\n",
    "print('Before:')\n",
    "print(ex[1]['triples'][2]['object'])\n",
    "print(ex[2]['triples'][2]['object'])\n",
    "print(ex[3]['triples'][2]['object'])\n",
    "for turn in ex[1:]:\n",
    "    turn['triples'][2]['object'].pop(2)\n",
    "print('After:')\n",
    "print(ex[1]['triples'][2]['object'])\n",
    "print(ex[2]['triples'][2]['object'])\n",
    "print(ex[3]['triples'][2]['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15bd44cd-b12c-445f-9b3b-e84bec51636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[{'token_list': ['check', 'the', 'engine'], 'group_num': '', 'tokens': 'check the engine', 'tokens_id': '4-6'}, {'token_list': ['It'], 'group_num': '', 'tokens': 'It', 'tokens_id': '10'}]\n"
     ]
    }
   ],
   "source": [
    "# Remove 'S2' from Request: ('S1', 'you')|('0', '19') request to#1 () ('check the engine', 'It', 'S2')|('4-6', '10', '14')\n",
    "ex = annotation[520]['with_token_anno'][1]['triples'][1]['object']\n",
    "print(len(ex))\n",
    "ex.pop(-1)\n",
    "print(annotation[520]['with_token_anno'][1]['triples'][1]['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2abfd556-9f61-42e8-ade6-39d1f1eee0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "{'tokens': '5 and as close as possible to the stage', 'token_list': ['5', 'and', 'as', 'close', 'as', 'possible', 'to', 'the', 'stage'], 'group_num': '', 'tokens_id': '21,23,27-33'}\n",
      "After:\n",
      "{'tokens': '5 as close as possible to the stage', 'token_list': ['5', 'as', 'close', 'as', 'possible', 'to', 'the', 'stage'], 'group_num': '', 'tokens_id': '21,27-33'}\n"
     ]
    }
   ],
   "source": [
    "ex = annotation[531]['with_token_anno'][2]['triples'][0]['object'][0]\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex['tokens_id'] = ex['tokens_id'].replace(',23', '')\n",
    "ex['tokens'] = ex['tokens'].replace(' and', '')\n",
    "ex['token_list'].pop(1)\n",
    "print('After:')\n",
    "print(annotation[531]['with_token_anno'][2]['triples'][0]['object'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e93b6c29-653e-42e3-b9b7-f13af379c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "{'tokens': 'We had a lot of fun there . There was so much joy and walk and breathe the winter air . We arrived at the park at 9 o ’ clock in the morning and didn ’ t leave till 3 o ’ clock in the afternoon We built a snowman , some snow dogs and one big snow fort . We went sliding on the nice hill . It was a real work out . At noon , we had the most special winter picnic outside', 'token_list': ['We', 'had', 'a', 'lot', 'of', 'fun', 'there', '.', 'There', 'was', 'so', 'much', 'joy', 'and', 'walk', 'and', 'breathe', 'the', 'winter', 'air', '.', 'We', 'arrived', 'at', 'the', 'park', 'at', '9', 'o', '’', 'clock', 'in', 'the', 'morning', 'and', 'didn', '’', 't', 'leave', 'till', '3', 'o', '’', 'clock', 'in', 'the', 'afternoon', 'We', 'built', 'a', 'snowman', ',', 'some', 'snow', 'dogs', 'and', 'one', 'big', 'snow', 'fort', '.', 'We', 'went', 'sliding', 'on', 'the', 'nice', 'hill', '.', 'It', 'was', 'a', 'real', 'work', 'out', '.', 'At', 'noon', ',', 'we', 'had', 'the', 'most', 'special', 'winter', 'picnic', 'outside'], 'group_num': '', 'tokens_id': '29-75,99-138'}\n",
      "After:\n",
      "{'tokens': 'We built a snowman , some snow dogs and one big snow fort . We went sliding on the nice hill . It was a real work out . At noon , we had the most special winter picnic outside', 'token_list': ['We', 'built', 'a', 'snowman', ',', 'some', 'snow', 'dogs', 'and', 'one', 'big', 'snow', 'fort', '.', 'We', 'went', 'sliding', 'on', 'the', 'nice', 'hill', '.', 'It', 'was', 'a', 'real', 'work', 'out', '.', 'At', 'noon', ',', 'we', 'had', 'the', 'most', 'special', 'winter', 'picnic', 'outside'], 'group_num': '', 'tokens_id': '99-138'}\n"
     ]
    }
   ],
   "source": [
    "print('Before:')\n",
    "print(annotation[590]['with_token_anno'][3]['triples'][3]['object'][1])\n",
    "tokens = \"We built a snowman , some snow dogs and one big snow fort . We went sliding on the nice hill . It was a real work out . At noon , we had the most special winter picnic outside\"\n",
    "annotation[590]['with_token_anno'][3]['triples'][3]['object'][1]['tokens'] = tokens\n",
    "annotation[590]['with_token_anno'][3]['triples'][3]['object'][1]['token_list'] = tokens.split(' ')\n",
    "annotation[590]['with_token_anno'][3]['triples'][3]['object'][1]['tokens_id'] = '99-138'\n",
    "\n",
    "annotation[590]['with_token_anno'][4]['triples'][3]['object'][1]['tokens'] = tokens\n",
    "annotation[590]['with_token_anno'][4]['triples'][3]['object'][1]['token_list'] = tokens.split(' ')\n",
    "annotation[590]['with_token_anno'][4]['triples'][3]['object'][1]['tokens_id'] = '99-138'\n",
    "\n",
    "print('After:')\n",
    "print(annotation[590]['with_token_anno'][3]['triples'][3]['object'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cc88e86-1bdc-4f14-8bce-4062fe50fa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'token_list': ['us'], 'group_num': '', 'tokens': 'us', 'tokens_id': '13'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}]\n",
      "After:\n",
      "[{'token_list': ['us'], 'group_num': '', 'tokens': 'us', 'tokens_id': '13'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}, {'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '15'}]\n"
     ]
    }
   ],
   "source": [
    "# Add 'S2' to ['us', 'S1']\n",
    "ex = annotation[769]['with_token_anno'][0]['triples'][0]['object']\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex.append({'tokens': 'S2', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '15'})\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a8fc0fb-c9c4-4741-80cc-bb426c795b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'tokens': 'mine', 'token_list': ['mine'], 'group_num': '', 'tokens_id': '54'}, {'tokens': 'my card', 'token_list': ['my', 'card'], 'group_num': '', 'tokens_id': '44-45'}, {'tokens': 'S2(my)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '47(44)'}]\n",
      "After:\n",
      "[{'tokens': 'mine', 'token_list': ['mine'], 'group_num': '', 'tokens_id': '54'}]\n",
      "\n",
      "Before:\n",
      "{'who': 'Both', 'eventType': 'implicit', 'subject': [{'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '20'}], 'object': [{'token_list': ['you'], 'group_num': '', 'tokens': 'you', 'tokens_id': '33'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}], 'predicate': {'tokens': 'meet', 'tokens_id': '', 'token_list': ['meet'], 'group_num': ''}, 'time': 'NOW', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Assemble', 'Meet_specifications', 'Make_acquaintance', 'Response', 'Meet_with_response', 'Locative_relation', 'Come_together'], 'frame_name': 'Assemble'}\n",
      "After:\n",
      "{'who': 'Both', 'eventType': 'implicit', 'subject': [{'token_list': ['S2'], 'group_num': '', 'tokens': 'S2', 'tokens_id': '20'}], 'object': [{'token_list': ['you'], 'group_num': '', 'tokens': 'you', 'tokens_id': '33'}, {'token_list': ['S1'], 'group_num': '', 'tokens': 'S1', 'tokens_id': '0'}], 'predicate': {'tokens': 'meet', 'tokens_id': '', 'token_list': ['meet'], 'group_num': ''}, 'time': 'NOW', 'polarity': 'pos', 'modality': 'actual', 'frame_candidates': ['Assemble', 'Meet_specifications', 'Make_acquaintance', 'Response', 'Meet_with_response', 'Locative_relation', 'Come_together'], 'frame_name': 'Make_acquaintance'}\n",
      "\n",
      "Before:\n",
      "[{'tokens': 'mine', 'token_list': ['mine'], 'group_num': '', 'tokens_id': '54'}, {'tokens': 'my card', 'token_list': ['my', 'card'], 'group_num': '', 'tokens_id': '44-45'}, {'tokens': 'S2(my)', 'token_list': ['S2'], 'group_num': '', 'tokens_id': '47(44)'}]\n",
      "After:\n",
      "[{'tokens': 'mine', 'token_list': ['mine'], 'group_num': '', 'tokens_id': '54'}]\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove ['my card,44-45', 'S2(my),47(44)'] from ['mine,54', 'my card,44-45', 'S2(my),47(44)']\n",
    "#     from triples[0]\n",
    "# 2. Change framename: \"Assemble\" to \"Make_acquaintance\" from triples[4] to match previous results\n",
    "# 3. Remove ['my card,44-45', 'S2(my),47(44)'] from ['mine,54', 'my card,44-45', 'S2(my),47(44)']\n",
    "#     from triples[8]\n",
    "ex = annotation[771]['with_token_anno'][3]['triples'][0]['object']\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex.pop(2)\n",
    "ex.pop(1)\n",
    "print('After:')\n",
    "print(ex)\n",
    "\n",
    "ex = annotation[771]['with_token_anno'][3]['triples'][4]\n",
    "print('\\nBefore:')\n",
    "print(ex)\n",
    "ex['frame_name'] = 'Make_acquaintance'\n",
    "print('After:')\n",
    "print(ex)\n",
    "\n",
    "ex = annotation[771]['with_token_anno'][3]['triples'][8]['object']\n",
    "print('\\nBefore:')\n",
    "print(ex)\n",
    "ex.pop(2)\n",
    "ex.pop(1)\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fc90eb8-8794-4f2a-a402-d174ca3c187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'tokens': 'enough cash', 'token_list': ['enough', 'cash'], 'group_num': '', 'tokens_id': '67,56'}]\n",
      "After:\n",
      "[{'tokens': 'enough', 'token_list': ['enough'], 'group_num': '', 'tokens_id': '67'}, {'tokens': 'cash', 'token_list': ['cash'], 'group_num': '', 'tokens_id': '56'}]\n"
     ]
    }
   ],
   "source": [
    "# Split the single entity as coreferences\n",
    "ex = annotation[832]['with_token_anno'][3]['triples'][2]['object']\n",
    "print('Before:')\n",
    "print(ex)\n",
    "ex[0]['tokens'] = 'enough'\n",
    "ex[0]['token_list'] = ['enough']\n",
    "ex[0]['tokens_id'] = '67'\n",
    "ex.append({\n",
    "    'tokens': 'cash',\n",
    "    'token_list': ['cash'],\n",
    "    'group_num': '',\n",
    "    'tokens_id': '56',\n",
    "})\n",
    "print('After:')\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f15389e-3301-4f48-854c-a0ef6c96508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "{'tokens': 'philosophy course', 'token_list': ['philosophy', 'course'], 'group_num': '', 'tokens_id': '8,25'}\n",
      "After:\n",
      "{'tokens': 'philosophy', 'token_list': ['philosophy'], 'group_num': '', 'tokens_id': '8'}\n",
      "Before:\n",
      "{'tokens': 'philosophy course', 'token_list': ['philosophy', 'course'], 'group_num': '', 'tokens_id': '8,25'}\n",
      "After:\n",
      "{'tokens': 'philosophy', 'token_list': ['philosophy'], 'group_num': '', 'tokens_id': '8'}\n",
      "Before:\n",
      "{'tokens': 'philosophy course', 'token_list': ['philosophy', 'course'], 'group_num': '', 'tokens_id': '8,25'}\n",
      "After:\n",
      "{'tokens': 'philosophy', 'token_list': ['philosophy'], 'group_num': '', 'tokens_id': '8'}\n"
     ]
    }
   ],
   "source": [
    "# Split the single entity as coreferences\n",
    "# Remove 'course' from 'philosophy course'\n",
    "for i in range(1, 4):\n",
    "    ex = annotation[914]['with_token_anno'][i]['triples'][0]['object'][1]\n",
    "    print('Before:')\n",
    "    print(ex)\n",
    "    ex['tokens'] = 'philosophy'\n",
    "    ex['token_list'] = ['philosophy']\n",
    "    ex['tokens_id'] = '8'\n",
    "    print('After:')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bf41baa-2757-4a85-99aa-818ea478eee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[{'tokens': 'drink', 'token_list': ['drink'], 'group_num': '', 'tokens_id': ''}, {'tokens': 'drink', 'tokens_id': '', 'token_list': ['drink'], 'group_num': ''}]\n",
      "After:\n",
      "[{'tokens': 'eat', 'token_list': ['eat'], 'group_num': '', 'tokens_id': ''}, {'tokens': 'eat', 'tokens_id': '', 'token_list': ['eat'], 'group_num': ''}]\n"
     ]
    }
   ],
   "source": [
    "# replace \"drink\" with \"eat\" in \"drink soup\"\n",
    "print('Before:')\n",
    "exs = [annotation[920]['with_token_anno'][i]['triples'][1]['predicate'] for i in range(2,len(annotation[920]['with_token_anno']))]\n",
    "print(exs)\n",
    "for ex in exs:\n",
    "    ex['tokens'] = 'eat'\n",
    "    ex['token_list'] = ['eat']\n",
    "print('After:')\n",
    "exs = [annotation[920]['with_token_anno'][i]['triples'][1]['predicate'] for i in range(2,len(annotation[920]['with_token_anno']))]\n",
    "print(exs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9accdcc-7745-41b1-963f-f8665e8eb80b",
   "metadata": {},
   "source": [
    "# Split objects/subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f8b2080-0bf9-4ac5-8bec-3e834da555e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "[(21, 2, 0, 'subject', 0), (21, 3, 0, 'subject', 0), (505, 0, 0, 'object', 0), (505, 1, 0, 'object', 0), (505, 2, 0, 'object', 0), (505, 2, 1, 'object', 0), (505, 2, 3, 'object', 0), (505, 3, 0, 'object', 0), (505, 3, 1, 'object', 0), (505, 3, 3, 'object', 0), (531, 2, 0, 'object', 0), (531, 3, 1, 'object', 0), (531, 4, 2, 'object', 0), (586, 2, 1, 'object', 0), (586, 3, 1, 'object', 0), (586, 4, 1, 'object', 0), (596, 0, 0, 'object', 0), (596, 1, 0, 'object', 0), (596, 2, 0, 'object', 0), (596, 3, 0, 'object', 0), (596, 4, 0, 'object', 0), (709, 3, 2, 'object', 0), (710, 0, 0, 'object', 0), (710, 1, 0, 'object', 0), (710, 1, 1, 'object', 0), (710, 2, 0, 'object', 0), (710, 2, 1, 'object', 0)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Fix Annotation Error:\n",
    "# Split objects (and subjects) to solve Case 3 (3)I.\n",
    "# There are 25 cases in objects and 2 cases in subjects.\n",
    "# Fortunately, there is no case where this happens in both subjects and objects.\n",
    "# objects = [{X}, {Y}, {A(B)}]\n",
    "    -- Case 1: dicts in the same object list --> X and Y are COREF\n",
    "    -- Case 2: the dict in the same object list that contains \"A(B)\" means\n",
    "                B is be a part of tokens in Y and B is the COREF of A.\n",
    "                Note (1): there might be some error cases when A/B are exchanged. (A is a part of Y)\n",
    "                Note (2): {A(B)} might have nothing to do with X.\n",
    "                Note (3): there might be {A(B)} and {C(D)}, where B and D might in X or/and Y.\n",
    "    -- Case 3: There are three forms of X's \"tokens_id\": \n",
    "                (1) \"num\"; (2) \"num1-num2\"; (3) \"num3,num4,num5\"\n",
    "               --> (3) means \n",
    "                   I. there are multiple objects for the same (subject, predicate), \n",
    "                       e.g. (\"S1\", \"S2\"), or (\"5\", \"as close as possible\").\n",
    "                   II. all tokens represent a meaning as all, \n",
    "                       e.g. (\"flight for the\", \"22\"), (\"the\", \"pie\").\n",
    "               Problem: \"tokens_id\" is separated by \",\", \n",
    "                   but the separation could not be identified in \"tokens\" and \"token_list\".\n",
    "               Problem: We want to keep case II, but split case I. \n",
    "                    --> manually check..., save the II idx to \"exceptions\"\n",
    "\n",
    "                The case of multiple objects is usually presented by different events \n",
    "                (sometimes with the same group id, but not always).\n",
    "                E.g. Evt1 = {'object': ['tokens_id': num3]}; Evt2 = {'object': ['tokens_id': num4]}\n",
    "\"\"\"\n",
    "fix_exs = []\n",
    "exceptions = [84, 388, 413, 500, 782, 832, 914]\n",
    "for idx, d in enumerate(annotation):\n",
    "    if idx in exceptions:\n",
    "        continue\n",
    "    for tid, turn in enumerate(d['with_token_anno']):\n",
    "        for trid, triple in enumerate(turn['triples']):\n",
    "            for eid, entity in enumerate(triple['subject']):\n",
    "                if ',' in entity['tokens_id']:\n",
    "                    fix_exs.append((idx, tid, trid, 'subject', eid))\n",
    "            for eid, entity in enumerate(triple['object']):\n",
    "                if ',' in entity['tokens_id']:\n",
    "                    fix_exs.append((idx, tid, trid, 'object', eid))\n",
    "print(len(fix_exs))\n",
    "print(fix_exs)\n",
    "\n",
    "for (idx, tid, trid, target, eid) in fix_exs:\n",
    "    triple = annotation[idx]['with_token_anno'][tid]['triples'][trid]\n",
    "    dialog = annotation[idx]['entry']['preprocess'].split()\n",
    "\n",
    "    tokens_id_lst = triple[target][eid]['tokens_id'].split(',')\n",
    "    for tokens_id in tokens_id_lst:\n",
    "        tokens_id = tokens_id.strip()\n",
    "        new_triple = copy.deepcopy(triple)\n",
    "        new_triple[target][eid]['tokens_id'] = tokens_id\n",
    "        if '-' in tokens_id:\n",
    "            soi, eoi = tokens_id.split('-') # if tokens_id is \"num1-num2\"\n",
    "            token_list = dialog[int(soi):int(eoi)+1]\n",
    "            new_triple[target][eid]['token_list'] = token_list\n",
    "            new_triple[target][eid]['tokens'] = ' '.join(token_list)\n",
    "        elif not re.findall('\\D', tokens_id): # if tokens_id is a single number\n",
    "            new_triple[target][eid]['token_list'] = dialog[int(tokens_id)]\n",
    "            new_triple[target][eid]['tokens'] = dialog[int(tokens_id)]\n",
    "        else:\n",
    "            print('Error')\n",
    "            print(idx, tid, trid, eid, target)\n",
    "            print(tokens_id_lst)\n",
    "        annotation[idx]['with_token_anno'][tid]['triples'].append(new_triple)\n",
    "    annotation[idx]['with_token_anno'][tid]['triples'].pop(trid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476d0ed-7587-4226-955d-54a3bf792637",
   "metadata": {},
   "source": [
    "# Adjust fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c2d74-9cbf-434e-b9d9-b548216a7868",
   "metadata": {},
   "source": [
    "## Get DailyDialog Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35b5ec0-7aaa-443c-ad52-23d3229a0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dailydialog_id2split.json') as f:\n",
    "    dailydialog_id2split = json.load(f) # {string: string}, e.g. {'0': train}\n",
    "id_map = json.load(open('../data/id_map.json')) # {string: int}, e.g. {'0': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6e3302c-efb5-4a23-a84a-682597213047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_index(split_dic, daily_idx, counter_dic):\n",
    "    split = split_dic[str(daily_idx)]\n",
    "    counter_dic[split] += 1\n",
    "    return split, counter_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7480e37-b3e9-4a33-82af-416e63d7dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "cnt = {'train': 0, 'valid': 0, 'test': 0}\n",
    "for d in annotation:\n",
    "    new_annotation = {}\n",
    "    new_annotation['source_id'] = d['entry']['source_id']\n",
    "    \n",
    "    daily_idx = id_map[d['entry']['source_id']]\n",
    "    split, cnt = get_split_index(dailydialog_id2split, daily_idx, cnt)\n",
    "    new_annotation['DailyDialog_id'] = daily_idx\n",
    "    new_annotation['split'] = split\n",
    "    new_annotation['dialog_id'] = cnt[split]\n",
    "    \n",
    "    new_annotation['dialogue'] = d['entry']['preprocess']\n",
    "    \n",
    "    dialog_events = []\n",
    "    remove_events = []\n",
    "    for turn in d['with_token_anno']:\n",
    "        if turn['checkEvent'] != 'hasEvent':\n",
    "            for event_idx, event in enumerate(turn['triples']):\n",
    "                remove_events.append(event)\n",
    "            dialog_events.append([])\n",
    "            continue\n",
    "            \n",
    "        new_evts_per_turn = []        \n",
    "        for event in turn['triples']:\n",
    "            if event in remove_events:\n",
    "                continue\n",
    "                \n",
    "            event_dict = {\n",
    "                'participants': {},\n",
    "                'event_status': {},\n",
    "                'event_info': {},\n",
    "            }\n",
    "            \n",
    "            event_dict['participants']['predicate'] = event['predicate']\n",
    "            event_dict['participants']['subjects'] = event['subject']\n",
    "            event_dict['participants']['objects'] = event['object']\n",
    "\n",
    "            event_dict['event_status']['polarity'] = 1 if event['polarity'] == 'pos' else 0\n",
    "            event_dict['event_status']['modality'] = 1 if event['modality'] == 'actual' else 0\n",
    "            event_dict['event_status']['time'] = event['time']\n",
    "            event_dict['event_status']['who'] = event.get('who', None)\n",
    "            \n",
    "            event_dict['event_info']['explicit'] = 1 if event['eventType'] == 'explicit' else 0\n",
    "            event_dict['event_info']['frame_name'] = event['frame_name']\n",
    "            \n",
    "            new_evts_per_turn.append(event_dict)\n",
    "        dialog_events.append(new_evts_per_turn)\n",
    "    assert len(dialog_events) == len(d['with_token_anno']), f\"len(dialog_events) {len(dialog_events)} should be {len(d['with_token_anno'])}\"\n",
    "    new_annotation['events'] = dialog_events\n",
    "    all_data.append(new_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f3aa6-af14-4817-b8a4-b3ded90c0d13",
   "metadata": {},
   "source": [
    "# Coreferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2cf3f-b137-49ac-8c89-0ed2f2d588db",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "Fields of original subject/object:\n",
    "```\n",
    "{\n",
    "        \"tokens\": Str,\n",
    "        \"token_list\": List, # list of tokens\n",
    "        \"group_num\": Str,\n",
    "        \"tokens_id\": Str\n",
    "}\n",
    "```\n",
    "\n",
    "Formats of \"tokens_id\":\n",
    " - \"14-15\"\n",
    " - \"17\"\n",
    " - \"60(32)\" # replace 32 with 60\n",
    " - \"6-7(31)\" # replace 31 with 6-7\n",
    " - \"21(4-5)\" # replace 4-5 with 21\n",
    " - \"52,54\" # not a continuous span, e.g. 782-2-0,832-3-2\n",
    " - \"57-59, 83\" # not a continuous span, e.g. 84-4-0\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90acac1e-3e11-409d-ae98-974c7cb229b6",
   "metadata": {},
   "source": [
    "## Remove \"S1\" coreference to \"S2\"\n",
    "Both \"S1\" and \"S2\" should not be in the same subjects/objects because the entities in the subjects/objects list means they are coreferences. Therefore I remove the \"S1\", \"S2\" from such case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "998e77e7-e2df-403e-b77a-245583204d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anno_id, d in enumerate(all_data):\n",
    "    for turn_id, turn in enumerate(d['events']):\n",
    "        for event_id, event in enumerate(turn):\n",
    "            tokens = [p['tokens'] for p in event['participants']['subjects']]\n",
    "            if 'S1' in tokens and 'S2' in tokens:\n",
    "                event['participants']['subjects'] = [p for p in event['participants']['subjects'] if p['tokens'] not in ['S1', 'S2']]\n",
    "            \n",
    "            tokens = [p['tokens'] for p in event['participants']['objects']]\n",
    "            if 'S1' in tokens and 'S2' in tokens:\n",
    "                event['participants']['objects'] = [p for p in event['participants']['objects'] if p['tokens'] not in ['S1', 'S2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593fc01-6401-451e-bd1e-4edddb7769d3",
   "metadata": {},
   "source": [
    "## Remove group annotations and make sure there is no extra empty token in each participant in order to get the correct coreference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f7857ab-c8a8-41cf-a732-dfc5aa92bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_format(participant, ex_id):\n",
    "    tokens = participant['tokens'].strip()\n",
    "    token_list = [token for token in participant['token_list'] if token]\n",
    "    tokens_id = participant['tokens_id'].strip()\n",
    "    if len(tokens) != len(participant['tokens']) or len(token_list) != len(participant['token_list']) \\\n",
    "     or len(tokens_id) != len(participant['tokens_id']):\n",
    "#         print(ex_id)\n",
    "#         print('before:', participant)\n",
    "        participant.update({\n",
    "            \"tokens\": tokens,\n",
    "            \"token_list\": token_list,\n",
    "            \"tokens_id\": tokens_id,\n",
    "        })\n",
    "#         print('after :', participant)\n",
    "#         print()\n",
    "    return participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d1b7175-9652-4e98-bb8a-f5096755783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anno_idx, d in enumerate(all_data):\n",
    "    for turn_idx, turn in enumerate(d['events']):\n",
    "        for event_idx, event in enumerate(turn):\n",
    "            # Remove group annotations\n",
    "            event['participants']['predicate']['tokens'] = re.sub('#[0-9]','', event['participants']['predicate']['tokens'])\n",
    "            event['participants']['predicate']['tokens_id'] = re.sub('#[0-9]','', event['participants']['predicate']['tokens_id'])\n",
    "            # check the format\n",
    "            event['participants']['predicate'] = check_format(event['participants']['predicate'], f\"{anno_idx}_{turn_idx}_{event_idx}\")\n",
    "            \n",
    "            for pid, p in enumerate(event['participants']['subjects']):\n",
    "                p['tokens'] = re.sub('#[0-9]','', p['tokens'])\n",
    "                p['tokens_id'] = re.sub('#[0-9]','', p['tokens_id'])\n",
    "            # check the format\n",
    "            event['participants']['subjects'] = [check_format(sbj, f\"{anno_idx}_{turn_idx}_{event_idx}\") for sbj in event['participants']['subjects']]\n",
    "\n",
    "            for p in event['participants']['objects']:\n",
    "                p['tokens'] = re.sub('#[0-9]','', p['tokens'])\n",
    "                p['tokens_id'] = re.sub('#[0-9]','', p['tokens_id'])\n",
    "            # check the format\n",
    "            event['participants']['objects'] = [check_format(obj, f\"{anno_idx}_{turn_idx}_{event_idx}\") for obj in event['participants']['objects']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbae967-c2b1-4952-b88b-edd3d7cdb444",
   "metadata": {},
   "source": [
    "## Get coreference clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e47f5208-d445-4c43-bff9-9338c9a02aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coref_cluster(entities, coreferences, visit):\n",
    "    if not any(entities):\n",
    "        return coreferences, visit\n",
    "    # Force 'S1' to be in the 1st cluster and 'S2' the 2nd\n",
    "    # check if any subject in subjects is already in the cluster\n",
    "    coref_id = None\n",
    "    for entity in entities:\n",
    "        if entity['tokens'] == 'S1':\n",
    "            coref_id = 0\n",
    "            if entity['tokens_id'] not in visit:\n",
    "                visit[entity['tokens_id']] = coref_id\n",
    "                coreferences[coref_id].append(entity['tokens_id'])\n",
    "            break\n",
    "        elif entity['tokens'] == 'S2':\n",
    "            coref_id = 1\n",
    "            if entity['tokens_id'] not in visit:\n",
    "                visit[entity['tokens_id']] = coref_id\n",
    "                coreferences[coref_id].append(entity['tokens_id'])\n",
    "            break\n",
    "        if entity['tokens_id'] in visit:\n",
    "            coref_id = visit[entity['tokens_id']]\n",
    "            break\n",
    "            \n",
    "    # add the subject_id to the cluster\n",
    "    for entity in entities:\n",
    "        tokens_id = entity['tokens_id']\n",
    "        if '(' in tokens_id:\n",
    "            add_to_coref_clusters = False\n",
    "            mention_id, replace_id = tokens_id.rstrip(')').split('(')\n",
    "            mention_token, replace_token = entity['tokens'].rstrip(')').split('(')\n",
    "            if mention_id in visit and replace_id not in visit:\n",
    "                coreferences[visit[mention_id]].append(replace_id)\n",
    "                visit[replace_id] = visit[mention_id]\n",
    "            elif replace_id in visit and mention_id not in visit:\n",
    "                coreferences[visit[replace_id]].append(mention_id)\n",
    "                visit[mention_id] = visit[replace_id]\n",
    "            elif mention_token == 'S1' or replace_token == 'S1':\n",
    "                coref_id = 0\n",
    "                if mention_id not in visit:\n",
    "                    visit[mention_id] = coref_id\n",
    "                    coreferences[coref_id].append(mention_id)\n",
    "                if replace_id not in visit:\n",
    "                    visit[replace_id] = coref_id\n",
    "                    coreferences[coref_id].append(replace_id)\n",
    "            elif mention_token == 'S2' or replace_token == 'S2':\n",
    "                coref_id = 1\n",
    "                if mention_id not in visit:\n",
    "                    visit[mention_id] = coref_id\n",
    "                    coreferences[coref_id].append(mention_id)\n",
    "                if replace_id not in visit:\n",
    "                    visit[replace_id] = coref_id\n",
    "                    coreferences[coref_id].append(replace_id)\n",
    "            else:\n",
    "                coref_id = len(coreferences)\n",
    "                visit[mention_id] = coref_id\n",
    "                visit[replace_id] = coref_id\n",
    "                coreferences.append([mention_id, replace_id])\n",
    "        elif coref_id is None and len(tokens_id) > 0:\n",
    "            coref_id = len(coreferences)\n",
    "            visit[tokens_id] = coref_id\n",
    "            coreferences.append([tokens_id])\n",
    "        elif coref_id is not None and tokens_id not in visit:\n",
    "            coreferences[coref_id].append(tokens_id)\n",
    "            visit[tokens_id] = coref_id\n",
    "    return coreferences, visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99dde32a-a85b-4f02-8732-6ada18efa43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id2token(participant, id2token):\n",
    "    for p in participant:\n",
    "        if '(' in p['tokens_id']:\n",
    "            id1, id2 = p['tokens_id'].rstrip(')').split('(')\n",
    "            tok1, tok2 = p['tokens'].rstrip(')').split('(')\n",
    "            if id1 not in id2token:\n",
    "                id2token[id1] = tok1\n",
    "            if id2 not in id2token:\n",
    "                id2token[id2] = tok2\n",
    "        elif p['tokens_id'] not in id2token:\n",
    "            id2token[p['tokens_id']] = p['tokens']\n",
    "    return id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "281e3889-ecff-4e82-beff-7f07be3f8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anno_id, d in enumerate(all_data):\n",
    "    d['coreferences'] = [[], []] # a list of list of clusters\n",
    "    d['tokensid2corefid'] = {} # tokens_id: coref_id (Str:Int)\n",
    "    d['tokensid2tokens'] = {}\n",
    "    for turn_id, turn in enumerate(d['events']):\n",
    "        for event_id, event in enumerate(turn):\n",
    "            d['coreferences'], d['tokensid2corefid'] = add_coref_cluster(event['participants']['subjects'], d['coreferences'], d['tokensid2corefid'])\n",
    "            d['coreferences'], d['tokensid2corefid'] = add_coref_cluster(event['participants']['objects'], d['coreferences'], d['tokensid2corefid'])\n",
    "            \n",
    "            d['tokensid2tokens'] = add_id2token(event['participants']['subjects'], d['tokensid2tokens'])\n",
    "            d['tokensid2tokens'] = add_id2token(event['participants']['objects'], d['tokensid2tokens'])\n",
    "            \n",
    "    dialogue_tokens = d['dialogue'].replace('\\n', ' ').split(' ')\n",
    "    d['coreferences_tokens'] = []\n",
    "    for cid, cluster in enumerate(d['coreferences']):\n",
    "        d['coreferences_tokens'].append([])\n",
    "        for tokens_id in cluster:\n",
    "            d['coreferences_tokens'][cid].append(d['tokensid2tokens'][tokens_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e3b783-856b-433f-b527-e8c4d1336bb0",
   "metadata": {},
   "source": [
    "## Replace entity list (subjects/objects) to a single entity (subject/object)\n",
    "The goal is to identify the same event regardless of status changes.\n",
    "\n",
    "Procedure:\n",
    "1. Select one representative entity from the entity list.\n",
    "    Criterion:\n",
    "    - The token_id is the nearest to the predicate's tokens_id\n",
    "    - In the case of implicit event where there is no predicate tokens_id, we select the token_id closest to the start token id of that turn.\n",
    "    - Contains \"S1\" or \"S2\"\n",
    "\n",
    "2. Replace the entity list (subjects/objects) to the dictionary of the selected entity, with additional \"entity_id\" field.\n",
    "```\n",
    "{\n",
    "    \"entity_id\": Int, # which is also the index of cluster in the \"coreferences\" field\n",
    "    \"tokens\": Str,\n",
    "    \"token_list\": List,\n",
    "    \"group_num\": Str,\n",
    "    \"tokens_id\": Str\n",
    "}\n",
    "```\n",
    "{\n",
    "        \"tokens\": Str,\n",
    "        \"token_list\": List, # list of tokens\n",
    "        \"group_num\": Str,\n",
    "        \"tokens_id\": Str\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33496dc4-64a9-4f4b-b925-2ecd06c1da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_lst = ['at', 'in', 'from', 'on', 'has', 'is', 'was', 'up', 'with', 'for', 'about', 'out', 'to', 'of', 'as', 'by', 'go']\n",
    "\n",
    "def get_span(participant):\n",
    "    if not participant['tokens_id'] or '(' in participant['tokens_id']:\n",
    "        return None, None\n",
    "    \n",
    "    start_idx, end_idx = None, None\n",
    "    \n",
    "    if ',' in tokens_id:\n",
    "        try:\n",
    "            id1, id2 = tokens_id.split(',')\n",
    "            if '-' in id1:\n",
    "                sub_id1, sub_id2 = get_span({'tokens_id': id1, 'token_list': participant['token_list'][:-1]})\n",
    "                return min(sub_id1, int(id2)), max(sub_id2, int(id2))\n",
    "            elif '-' in id2:\n",
    "                sub_id1, sub_id2 = get_span({'tokens_id': id2, 'token_list': participant['token_list'][1:]})\n",
    "                return min(sub_id1, int(id1)), max(sub_id2, int(id1))\n",
    "            token1, token2 = participant['token_list']\n",
    "            if token1 in omit_lst:\n",
    "                return get_span({'tokens_id': id2, 'token_list': token2})\n",
    "            elif token2 in omit_lst:\n",
    "                return get_span({'tokens_id': id1, 'token_list': token1})\n",
    "            else:\n",
    "                return int(id1), int(id2)\n",
    "        except:\n",
    "            import ipdb;ipdb.set_trace()\n",
    "    else:\n",
    "        arg_ids = tokens_id.split('-')\n",
    "        try:\n",
    "            if len(arg_ids) == 1:\n",
    "                start_idx, end_idx = int(arg_ids[0]), int(arg_ids[0])\n",
    "            elif len(arg_ids) == 2:\n",
    "                start_idx, end_idx = int(arg_ids[0]), int(arg_ids[1])\n",
    "            return start_idx, end_idx\n",
    "        except:\n",
    "            import ipdb;ipdb.set_trace()\n",
    "    if not start_idx:\n",
    "        import ipdb;ipdb.set_trace()\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def select_representative_entity(entity_type, entity_lst, verb, turn_start, turn_end):\n",
    "    if len(entity_lst) == 1:\n",
    "        return entity_lst[0]\n",
    "    \n",
    "    verb_start, verb_end = get_span(verb)\n",
    "    \n",
    "    min_distance = 1000\n",
    "    selected_entity = None\n",
    "    \n",
    "    for entity in entity_lst:\n",
    "        tokens_id = entity['tokens_id']\n",
    "        if '(' in tokens_id:\n",
    "            continue\n",
    "        elif ',' in tokens_id:\n",
    "            id1, id2 = tokens_id.split(',')\n",
    "            try:\n",
    "                max_id = max(int(id1), int(id2))\n",
    "            except:\n",
    "                try:\n",
    "                    max_id = max(int(id1.strip().split('-')[-1]), int(id2.strip().split('-')[-1]))\n",
    "                except:\n",
    "                    import ipdb;ipdb.set_trace()\n",
    "            \n",
    "            if max_id > turn_end:\n",
    "                continue\n",
    "            \n",
    "            if verb_start:\n",
    "                # explicit event\n",
    "                if entity_type == 'sbj':\n",
    "                    distance = abs(verb_start - max_id)\n",
    "                elif entity_type == 'obj':\n",
    "                    distance = abs(max_id - verb_end)\n",
    "            else:\n",
    "                # implicit event\n",
    "                if entity_type == 'sbj':\n",
    "                    distance = abs(max_id - turn_start)\n",
    "                elif entity_type == 'obj':\n",
    "                    distance = abs(turn_end - max_id)\n",
    "        else:\n",
    "            entity_start, entity_end = get_span(entity)\n",
    "            if verb_start:\n",
    "                if entity_type == 'sbj':\n",
    "                    distance = abs(entity_end - verb_start)\n",
    "                elif entity_type == 'obj':\n",
    "                    distance = abs(entity_start - verb_end)\n",
    "            else:\n",
    "                # implicit event\n",
    "                if entity_type == 'sbj':\n",
    "                    distance = abs(entity_start - turn_start)\n",
    "                elif entity_type == 'obj':\n",
    "                    distance = abs(turn_end - entity_end)\n",
    "                \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            selected_entity = entity\n",
    "    return selected_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa6b07cc-b84b-451a-9d0b-a55f09aa8cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1002"
     ]
    }
   ],
   "source": [
    "for anno_id, d in enumerate(all_data):\n",
    "    print('\\r', anno_id, end='')\n",
    "    sents = d['dialogue'].split('\\n')\n",
    "    sent_lens = [len(sent.split(' ')) for sent in sents]\n",
    "    start_token_ids = np.cumsum([0] + sent_lens[:-1])\n",
    "    \n",
    "    for turn_id, turn in enumerate(d['events']):\n",
    "        start_turn_token_id = start_token_ids[turn_id]\n",
    "        end_turn_token_id = start_token_ids[turn_id+1] - 1\n",
    "            \n",
    "        for event_id, event in enumerate(turn):\n",
    "            # Subject\n",
    "            selected_subject = select_representative_entity('sbj', event['participants']['subjects'], event['participants']['predicate'], start_turn_token_id, end_turn_token_id)\n",
    "            if not selected_subject:\n",
    "                print(anno_id, turn_id, )\n",
    "                print(event['participants']['subjects'], event['participants']['predicate'], start_turn_token_id, end_turn_token_id)\n",
    "            event['participants']['subject'] = {\n",
    "                \"entity_id\": d['tokensid2corefid'][selected_subject['tokens_id']],\n",
    "            }\n",
    "            event['participants']['subject'].update(selected_subject)\n",
    "            \n",
    "            # Object\n",
    "            event['participants']['object'] = {}\n",
    "            if event['participants']['objects'][0]['tokens_id']:\n",
    "                selected_object = select_representative_entity('obj', event['participants']['objects'], event['participants']['predicate'], start_turn_token_id, end_turn_token_id)\n",
    "                event['participants']['object']['entity_id'] = d['tokensid2corefid'][selected_object['tokens_id']]\n",
    "                event['participants']['object'].update(selected_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb786ecc-b2da-4eac-b652-1185cca82e2e",
   "metadata": {},
   "source": [
    "## Obtain predicate class from its lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2952e727-a2ed-43e7-a4af-cd520331d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1db0e533-1f54-4872-9ce7-30bed26bae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_preposition(predicate):\n",
    "    remove_postfix = ['to', 'for', 'with', 'up', 'at', 'from', 'about', 'out', 'by', 'in', 'of', 'as', 'by']\n",
    "    remove_postfix = ' ' + '| '.join(remove_postfix)\n",
    "    return re.sub(f'({remove_postfix})$', '', predicate.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ea49478-fb00-46d9-aeb1-032e5db36e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anno_id, d in enumerate(all_data):\n",
    "    for turn_id, turn in enumerate(d['events']):\n",
    "        for event_id, event in enumerate(turn):            \n",
    "            verb_tokens = remove_preposition(event['participants']['predicate']['tokens'])\n",
    "            verb_class = ' '.join([token.lemma_ for token in spacy_model(verb_tokens)])\n",
    "            event['event_info']['predicate_class'] = verb_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54bfca0-c024-4ae3-8335-7698cade8860",
   "metadata": {},
   "source": [
    "# Save new annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ae97610-d43b-4bbc-8156-cca9f18107b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/annotations.pickle', 'wb') as f:\n",
    "#     pickle.dump(annotation, f)\n",
    "    pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3ab03eb-080e-4cd4-987e-5c488be124ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['source_id', 'DailyDialog_id', 'split', 'dialog_id', 'dialogue', 'events', 'coreferences', 'tokensid2corefid', 'tokensid2tokens', 'coreferences_tokens'])\n"
     ]
    }
   ],
   "source": [
    "print(all_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f94585d-81bf-477a-827d-555ff1e2c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/annotation_train.jsonl', 'w') as train_f, open('../data/annotation_valid.jsonl', 'w') as valid_f,  open('../data/annotation_test.jsonl', 'w') as test_f:\n",
    "    for anno_id, d in enumerate(all_data):\n",
    "        for turn_id, turn in enumerate(d['events']):\n",
    "            for event_id, event in enumerate(turn):\n",
    "                del event['participants']['subjects']\n",
    "                del event['participants']['objects']\n",
    "        del d['source_id']\n",
    "        del d['tokensid2corefid']\n",
    "        del d['tokensid2tokens']\n",
    "        if d['split'] == 'train':\n",
    "            train_f.write(json.dumps(d) + '\\n')\n",
    "        elif d['split'] == 'valid':\n",
    "            valid_f.write(json.dumps(d) + '\\n')\n",
    "        elif d['split'] == 'test':\n",
    "            test_f.write(json.dumps(d) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7190f-b7ae-4f53-b2ca-0ebbdeaa4224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
